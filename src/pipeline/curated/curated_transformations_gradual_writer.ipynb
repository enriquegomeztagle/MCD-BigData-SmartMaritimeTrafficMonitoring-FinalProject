{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcee126-0b13-4b29-b143-90bee665a886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, time, re, shlex, subprocess\n",
    "from typing import List, Optional\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeb8eb-c784-4990-9826-5f5ba6679c81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_BASE = \"gs://bucket20250825maestria/AIS_2024_raw\"\n",
    "OUTPUT_BASE = \"gs://bucket20250825maestria/AIS_2024_curated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c23010-44db-471b-bf88-75d35332e56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MONTHS = [\"2024-08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce4ab0-6a7b-42bf-beca-d9d2d80d809c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAVE_MODE = \"overwrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935ecb6-4e67-47a5-8e7f-df1f3d70ab6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_FILES_PER_PARTITION = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f0536-4848-4787-9d17-5ea94b398141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHUFFLE_PARTITIONS = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dfb233-b695-4e6f-b81d-469bc70a786b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESUME_WITH_MARKERS = True\n",
    "SKIP_IF_PARTITION_EXISTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7fdb1-6793-4203-9c29-8cb62619573d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ARCHIVE_GCS = \"gs://bucket20250825maestria/envs/pygeo-venv.tar.gz#environment\"\n",
    "PY_IN_ENV = \"./environment/venv/bin/python\"\n",
    "WHEEL = \"gs://bucket20250825maestria/wheels/pygeohash-1.2.0.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9f257-3ec0-475e-b42c-bbf562cf61f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hadoop_path_exists(spark: SparkSession, path: str) -> bool:\n",
    "    jvm = spark._jvm\n",
    "    sc = spark.sparkContext\n",
    "    fs = jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        jvm.java.net.URI(path), sc._jsc.hadoopConfiguration()\n",
    "    )\n",
    "    return fs.exists(jvm.org.apache.hadoop.fs.Path(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ee1ebe-93ac-4212-9a86-f4e9fc63e8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gsutil_prefix_exists(path: str) -> bool:\n",
    "    try:\n",
    "        p = subprocess.run(\n",
    "            f\"gsutil ls -d {shlex.quote(path)}\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        if p.returncode == 0 and p.stdout.strip():\n",
    "            return True\n",
    "        p2 = subprocess.run(\n",
    "            f\"gsutil ls {shlex.quote(path)}**\",\n",
    "            shell=True,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "        )\n",
    "        return p2.returncode == 0 and bool(p2.stdout.strip())\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25aac9-5002-492e-a05d-c1402464864c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def partition_exists(\n",
    "    spark: SparkSession, base_out: str, part_col: str, part_val: str\n",
    ") -> bool:\n",
    "    return hadoop_path_exists(spark, f\"{base_out.rstrip('/')}/{part_col}={part_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada1a72-24b5-4426-a9b5-e1af54e23698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def marker_path(base_out: str, part_col: str, part_val: str) -> str:\n",
    "    return f\"{base_out.rstrip('/')}/_markers/{part_col}={part_val}/_SUCCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba13ff-f394-4c47-a62b-707aa3f7ec34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def marker_exists(\n",
    "    spark: SparkSession, base_out: str, part_col: str, part_val: str\n",
    ") -> bool:\n",
    "    return hadoop_path_exists(spark, marker_path(base_out, part_col, part_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ee9fb-9602-421d-89be-5301044fd371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_marker(\n",
    "    spark: SparkSession, base_out: str, part_col: str, part_val: str\n",
    ") -> None:\n",
    "    spark.sparkContext.parallelize([\"ok\"], 1).saveAsTextFile(\n",
    "        marker_path(base_out, part_col, part_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e62afa-2eb3-4726-b576-560f6b8cd8c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def month_input_path(month: str) -> str:\n",
    "    return f\"{INPUT_BASE.rstrip('/')}/ym={month}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4f98e-3b26-4d40-a17a-8d11fe3049ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05928198-b3c4-40b7-9757-8cf1076966f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev = SparkSession.getActiveSession()\n",
    "if prev is not None:\n",
    "    try:\n",
    "        prev.stop()\n",
    "    except Exception as e:\n",
    "        print(\"WARN cerrando sesión previa:\", e)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc70ec-c710-46ce-9be3-cd92c03f4268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"curated-writer-gradual\")\n",
    "    .master(\"yarn\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.yarn.unmanagedAM.enabled\", \"false\")\n",
    "    .config(\"spark.yarn.dist.archives\", ARCHIVE_GCS)\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", PY_IN_ENV)\n",
    "    .config(\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\", PY_IN_ENV)\n",
    "    .config(\"spark.network.timeout\", \"800s\")\n",
    "    .config(\"spark.shuffle.io.maxRetries\", \"10\")\n",
    "    .config(\"spark.shuffle.io.retryWait\", \"10s\")\n",
    "    .config(\"spark.reducer.maxReqsInFlight\", \"1\")\n",
    "    .config(\"spark.stage.maxConsecutiveAttempts\", \"10\")\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.scheduler.excludeOnFailure\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", str(SHUFFLE_PARTITIONS))\n",
    "    .config(\"spark.speculation\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec36daa-8278-4c84-b673-3e93ab8ef7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "sc._jsc.hadoopConfiguration().set(\n",
    "    \"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5ab6e-b710-4929-a909-d72a21c93095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile(WHEEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f40296-cae6-4b50-b8eb-2753cc145d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"AppId:\", sc.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14225088-8c7f-4c57-87bf-3b8126f4bc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _make_geohash_pudf(precision: int):\n",
    "    @pandas_udf(\"string\")\n",
    "    def _encode(lat: pd.Series, lon: pd.Series) -> pd.Series:\n",
    "        import pygeohash as pgh\n",
    "\n",
    "        return pd.Series(\n",
    "            [\n",
    "                (\n",
    "                    pgh.encode(la, lo, precision=precision)\n",
    "                    if pd.notnull(la) and pd.notnull(lo)\n",
    "                    else None\n",
    "                )\n",
    "                for la, lo in zip(lat, lon)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    _encode.__name__ = f\"geohash_p{precision}\"\n",
    "    return _encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74862d-e4a5-4a7b-b1fa-5481c493508b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_curated_transformations(df: DataFrame) -> DataFrame:\n",
    "    t0 = time.time()\n",
    "\n",
    "    def _log(msg: str, sdf: DataFrame | None = None):\n",
    "        print(f\"[curated] {msg}\")\n",
    "        if sdf is not None:\n",
    "            try:\n",
    "                print(f\"[curated] columnas={len(sdf.columns)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[curated] (warn) no se pudo imprimir schema: {e}\")\n",
    "\n",
    "    _log(\"inicio pipeline curated\", df)\n",
    "\n",
    "    df1 = (\n",
    "        df.withColumn(\n",
    "            \"MMSI\", F.regexp_extract(F.col(\"MMSI\").cast(\"string\"), r\"(\\d{1,9})$\", 1)\n",
    "        )\n",
    "        .withColumn(\"MMSI\", F.lpad(\"MMSI\", 9, \"0\"))\n",
    "        .withColumn(\n",
    "            \"BaseDateTime\",\n",
    "            F.to_timestamp(F.col(\"BaseDateTime\"), \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
    "        )\n",
    "        .withColumn(\"LAT\", F.col(\"LAT\").cast(\"double\"))\n",
    "        .withColumn(\"LON\", F.col(\"LON\").cast(\"double\"))\n",
    "        .withColumn(\"SOG\", F.col(\"SOG\").cast(\"double\"))\n",
    "        .withColumn(\"COG\", F.col(\"COG\").cast(\"double\"))\n",
    "        .withColumn(\"Heading\", F.col(\"Heading\").cast(\"double\"))\n",
    "        .withColumn(\"Length\", F.col(\"Length\").cast(\"double\"))\n",
    "        .withColumn(\"Width\", F.col(\"Width\").cast(\"double\"))\n",
    "        .withColumn(\"Draft\", F.col(\"Draft\").cast(\"double\"))\n",
    "        .withColumn(\"VesselName\", F.trim(F.col(\"VesselName\")))\n",
    "        .withColumn(\"IMO\", F.trim(F.col(\"IMO\")))\n",
    "        .withColumn(\"CallSign\", F.trim(F.col(\"CallSign\")))\n",
    "        .withColumn(\"VesselType\", F.trim(F.col(\"VesselType\")))\n",
    "        .withColumn(\"Cargo\", F.trim(F.col(\"Cargo\")))\n",
    "        .withColumn(\"TransceiverClass\", F.upper(F.trim(F.col(\"TransceiverClass\"))))\n",
    "    )\n",
    "    _log(\"df1: casts y normalizaciones base aplicadas\", df1)\n",
    "\n",
    "    wrap_lon = (\n",
    "        F.when(F.col(\"LON\") > 180, F.col(\"LON\") - 360)\n",
    "        .when(F.col(\"LON\") < -180, F.col(\"LON\") + 360)\n",
    "        .otherwise(F.col(\"LON\"))\n",
    "    )\n",
    "    df2 = (\n",
    "        df1.withColumn(\n",
    "            \"LAT\",\n",
    "            F.when((F.col(\"LAT\") >= -90) & (F.col(\"LAT\") <= 90), F.round(\"LAT\", 5)),\n",
    "        )\n",
    "        .withColumn(\"LON\", F.round(wrap_lon, 5))\n",
    "        .filter(F.col(\"LAT\").isNotNull() & F.col(\"LON\").isNotNull())\n",
    "    )\n",
    "    _log(\"df2: coordenadas corregidas/recortadas y nulos filtrados\", df2)\n",
    "\n",
    "    df3 = (\n",
    "        df2.withColumn(\n",
    "            \"Heading\", F.when(F.col(\"Heading\") == 511, None).otherwise(F.col(\"Heading\"))\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"Heading\",\n",
    "            F.when(F.col(\"Heading\").isNotNull(), F.col(\"Heading\") % 360).otherwise(\n",
    "                None\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"COG\", F.when(F.col(\"COG\").isNotNull(), F.col(\"COG\") % 360).otherwise(None)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"SOG\",\n",
    "            F.when((F.col(\"SOG\") >= 0) & (F.col(\"SOG\") <= 70), F.col(\"SOG\")).otherwise(\n",
    "                None\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"IMO\",\n",
    "            F.when(F.col(\"IMO\").isin(\"IMO0000000\", \"0\", \"\"), None).otherwise(\n",
    "                F.col(\"IMO\")\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"Length\",\n",
    "            F.when((F.col(\"Length\") >= 1) & (F.col(\"Length\") <= 450), F.col(\"Length\")),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"Width\",\n",
    "            F.when((F.col(\"Width\") >= 1) & (F.col(\"Width\") <= 70), F.col(\"Width\")),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"Draft\",\n",
    "            F.when((F.col(\"Draft\") >= 0) & (F.col(\"Draft\") <= 25), F.col(\"Draft\")),\n",
    "        )\n",
    "    )\n",
    "    _log(\"df3: reglas de rango y normalizaciones aplicadas\", df3)\n",
    "\n",
    "    df4 = df3.withColumn(\n",
    "        \"VesselTypeInt\",\n",
    "        F.when(\n",
    "            F.col(\"VesselType\").rlike(r\"^\\d+$\"), F.col(\"VesselType\").cast(\"int\")\n",
    "        ).otherwise(None),\n",
    "    ).withColumn(\"VesselTypeCode\", F.col(\"VesselType\"))\n",
    "\n",
    "    type_map = {\n",
    "        0: \"Not available (default)\",\n",
    "        20: \"Wing in ground (WIG), all ships of this type\",\n",
    "        21: \"Wing in ground (WIG), Hazardous category A\",\n",
    "        22: \"Wing in ground (WIG), Hazardous category B\",\n",
    "        23: \"Wing in ground (WIG), Hazardous category C\",\n",
    "        24: \"Wing in ground (WIG), Hazardous category D\",\n",
    "        25: \"Wing in ground (WIG), Reserved for future use\",\n",
    "        26: \"Wing in ground (WIG), Reserved for future use\",\n",
    "        27: \"Wing in ground (WIG), Reserved for future use\",\n",
    "        28: \"Wing in ground (WIG), Reserved for future use\",\n",
    "        29: \"Wing in ground (WIG), Reserved for future use\",\n",
    "        30: \"Fishing\",\n",
    "        31: \"Towing\",\n",
    "        32: \"Towing: length exceeds 200m or breadth exceeds 25m\",\n",
    "        33: \"Dredging or underwater ops\",\n",
    "        34: \"Diving ops\",\n",
    "        35: \"Military ops\",\n",
    "        36: \"Sailing\",\n",
    "        37: \"Pleasure Craft\",\n",
    "        38: \"Reserved\",\n",
    "        39: \"Reserved\",\n",
    "        40: \"High speed craft (HSC), all ships of this type\",\n",
    "        41: \"High speed craft (HSC), Hazardous category A\",\n",
    "        42: \"High speed craft (HSC), Hazardous category B\",\n",
    "        43: \"High speed craft (HSC), Hazardous category C\",\n",
    "        44: \"High speed craft (HSC), Hazardous category D\",\n",
    "        45: \"High speed craft (HSC), Reserved for future use\",\n",
    "        46: \"High speed craft (HSC), Reserved for future use\",\n",
    "        47: \"High speed craft (HSC), Reserved for future use\",\n",
    "        48: \"High speed craft (HSC), Reserved for future use\",\n",
    "        49: \"High speed craft (HSC), No additional information\",\n",
    "        50: \"Pilot Vessel\",\n",
    "        51: \"Search and Rescue vessel\",\n",
    "        52: \"Tug\",\n",
    "        53: \"Port Tender\",\n",
    "        54: \"Anti-pollution equipment\",\n",
    "        55: \"Law Enforcement\",\n",
    "        56: \"Spare - Local Vessel\",\n",
    "        57: \"Spare - Local Vessel\",\n",
    "        58: \"Medical Transport\",\n",
    "        59: \"Noncombatant ship according to RR Resolution No. 18\",\n",
    "        60: \"Passenger, all ships of this type\",\n",
    "        61: \"Passenger, Hazardous category A\",\n",
    "        62: \"Passenger, Hazardous category B\",\n",
    "        63: \"Passenger, Hazardous category C\",\n",
    "        64: \"Passenger, Hazardous category D\",\n",
    "        65: \"Passenger, Reserved for future use\",\n",
    "        66: \"Passenger, Reserved for future use\",\n",
    "        67: \"Passenger, Reserved for future use\",\n",
    "        68: \"Passenger, Reserved for future use\",\n",
    "        69: \"Passenger, No additional information\",\n",
    "        70: \"Cargo, all ships of this type\",\n",
    "        71: \"Cargo, Hazardous category A\",\n",
    "        72: \"Cargo, Hazardous category B\",\n",
    "        73: \"Cargo, Hazardous category C\",\n",
    "        74: \"Cargo, Hazardous category D\",\n",
    "        75: \"Cargo, Reserved for future use\",\n",
    "        76: \"Cargo, Reserved for future use\",\n",
    "        77: \"Cargo, Reserved for future use\",\n",
    "        78: \"Cargo, Reserved for future use\",\n",
    "        79: \"Cargo, No additional information\",\n",
    "        80: \"Tanker, all ships of this type\",\n",
    "        81: \"Tanker, Hazardous category A\",\n",
    "        82: \"Tanker, Hazardous category B\",\n",
    "        83: \"Tanker, Hazardous category C\",\n",
    "        84: \"Tanker, Hazardous category D\",\n",
    "        85: \"Tanker, Reserved for future use\",\n",
    "        86: \"Tanker, Reserved for future use\",\n",
    "        87: \"Tanker, Reserved for future use\",\n",
    "        88: \"Tanker, Reserved for future use\",\n",
    "        89: \"Tanker, No additional information\",\n",
    "        90: \"Other Type, all ships of this type\",\n",
    "        91: \"Other Type, Hazardous category A\",\n",
    "        92: \"Other Type, Hazardous category B\",\n",
    "        93: \"Other Type, Hazardous category C\",\n",
    "        94: \"Other Type, Hazardous category D\",\n",
    "        95: \"Other Type, Reserved for future use\",\n",
    "        96: \"Other Type, Reserved for future use\",\n",
    "        97: \"Other Type, Reserved for future use\",\n",
    "        98: \"Other Type, Reserved for future use\",\n",
    "        99: \"Other Type, no additional information\",\n",
    "    }\n",
    "    mapping_expr = F.create_map(\n",
    "        *[x for kv in type_map.items() for x in (F.lit(int(kv[0])), F.lit(kv[1]))]\n",
    "    )\n",
    "\n",
    "    cls = (\n",
    "        F.when(F.col(\"VesselTypeInt\").between(20, 29), \"WIG\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(30, 39), \"Small/Leisure\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(40, 49), \"HSC\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(50, 59), \"Service/Special\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(60, 69), \"Passenger\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(70, 79), \"Cargo\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(80, 89), \"Tanker\")\n",
    "        .when(F.col(\"VesselTypeInt\").between(90, 99), \"Other\")\n",
    "        .otherwise(\"Unspecified\")\n",
    "    )\n",
    "\n",
    "    df5 = df4.withColumn(\n",
    "        \"VesselTypeName\", mapping_expr[F.col(\"VesselTypeInt\")]\n",
    "    ).withColumn(\"VesselTypeClass\", cls)\n",
    "    _log(\"df5: enriquecimiento de VesselType (Int/Name/Class)\", df5)\n",
    "\n",
    "    status_map = [\n",
    "        (0, \"Under way using engine\"),\n",
    "        (1, \"At anchor\"),\n",
    "        (2, \"Not under command\"),\n",
    "        (3, \"Restricted manoeuverability\"),\n",
    "        (4, \"Constrained by her draught\"),\n",
    "        (5, \"Moored\"),\n",
    "        (6, \"Aground\"),\n",
    "        (7, \"Engaged in fishing\"),\n",
    "        (8, \"Under way sailing\"),\n",
    "        (14, \"AIS-SART/MOB/EPIRB active\"),\n",
    "        (15, \"Not defined (default)\"),\n",
    "    ]\n",
    "    status_df = df.sparkSession.createDataFrame(\n",
    "        status_map, \"NavStatusInt INT, NavStatusName STRING\"\n",
    "    )\n",
    "    df6 = (\n",
    "        df5.withColumn(\"NavStatusInt\", F.col(\"Status\").cast(\"int\"))\n",
    "        .join(F.broadcast(status_df), on=\"NavStatusInt\", how=\"left\")\n",
    "        .withColumn(\n",
    "            \"NavStatusName\",\n",
    "            F.when(F.col(\"NavStatusInt\").isNull(), \"Not reported\")\n",
    "            .when(~F.col(\"NavStatusInt\").between(0, 15), \"Unknown code\")\n",
    "            .otherwise(F.col(\"NavStatusName\")),\n",
    "        )\n",
    "    )\n",
    "    _log(\"df6: join catálogo estatus navegación\", df6)\n",
    "\n",
    "    def _normalize(cname: str):\n",
    "        return F.when(\n",
    "            F.col(cname).isNotNull(),\n",
    "            F.regexp_replace(\n",
    "                F.regexp_replace(F.upper(F.trim(F.col(cname))), r\"[^A-Z0-9 ]\", \"\"),\n",
    "                r\"\\s+\",\n",
    "                \" \",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    df7 = df6.withColumn(\n",
    "        \"VesselName\", F.coalesce(_normalize(\"VesselName\"), F.col(\"VesselName\"))\n",
    "    ).withColumn(\"CallSign\", F.coalesce(_normalize(\"CallSign\"), F.col(\"CallSign\")))\n",
    "    _log(\"df7: normalización nombres/callsign\", df7)\n",
    "\n",
    "    df8 = (\n",
    "        df7.withColumn(\"ym\", F.date_format(\"BaseDateTime\", \"yyyy-MM\"))\n",
    "        .withColumn(\"date\", F.to_date(\"BaseDateTime\"))\n",
    "        .withColumn(\"hour\", F.hour(\"BaseDateTime\"))\n",
    "        .withColumn(\"dow\", F.date_format(\"BaseDateTime\", \"E\"))\n",
    "        .withColumn(\"week\", F.weekofyear(\"BaseDateTime\"))\n",
    "        .withColumn(\"month\", F.month(\"BaseDateTime\"))\n",
    "        .withColumn(\"quarter\", F.quarter(\"BaseDateTime\"))\n",
    "        .withColumn(\"SOG_ms\", F.col(\"SOG\") * 0.514444)\n",
    "    )\n",
    "    _log(\"df8: derivadas temporales\", df8)\n",
    "\n",
    "    gh9 = _make_geohash_pudf(9)\n",
    "    df9 = df8.withColumn(\"geohash9\", gh9(F.col(\"LAT\"), F.col(\"LON\")))\n",
    "    _log(\"df9: geohash9\", df9)\n",
    "\n",
    "    df10 = df9.dropDuplicates([\"MMSI\", \"BaseDateTime\"])\n",
    "    _log(\"df10: deduplicación final\", df10)\n",
    "\n",
    "    print(f\"[curated] fin (elapsed={time.time()-t0:0.2f}s)\")\n",
    "    return df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb71cd-0f73-402e-82da-0d75e2d663b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed, skipped, failed = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc5738-0dfe-40c5-92ba-10f26ec33e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SAVE_MODE == \"overwrite\":\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd57f71-c24c-4200-80fd-f612e5d87aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for m in sorted(MONTHS):\n",
    "    in_path = month_input_path(m)\n",
    "    exists = gsutil_prefix_exists(in_path)\n",
    "    if not exists:\n",
    "        print(f\"[SKIP] No existe entrada {in_path}\")\n",
    "        skipped.append(m)\n",
    "        continue\n",
    "\n",
    "    if RESUME_WITH_MARKERS and marker_exists(spark, OUTPUT_BASE, \"ym\", m):\n",
    "        print(f\"[SKIP/MARKER] ym={m} ya tiene marker.\")\n",
    "        skipped.append(m)\n",
    "        continue\n",
    "    if SKIP_IF_PARTITION_EXISTS and partition_exists(spark, OUTPUT_BASE, \"ym\", m):\n",
    "        print(f\"[SKIP/EXISTS] ym={m} ya existe en salida.\")\n",
    "        skipped.append(m)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"\\n=== ym={m} ===\")\n",
    "        print(f\"[READ] {in_path}\")\n",
    "        df = spark.read.parquet(in_path)\n",
    "\n",
    "        print(\"[XFORM] apply_curated_transformations...\")\n",
    "        df_t = apply_curated_transformations(df)\n",
    "        if \"ym\" not in df_t.columns:\n",
    "            df_t = df_t.withColumn(\"ym\", F.lit(m))\n",
    "\n",
    "        df_part = df_t.repartition(max(32, SHUFFLE_PARTITIONS // 2)).coalesce(\n",
    "            TARGET_FILES_PER_PARTITION\n",
    "        )\n",
    "\n",
    "        print(f\"[WRITE] {OUTPUT_BASE}  (ym={m}, mode={SAVE_MODE})\")\n",
    "        (\n",
    "            df_part.write.mode(SAVE_MODE)\n",
    "            .option(\"compression\", \"snappy\")\n",
    "            .partitionBy(\"ym\")\n",
    "            .parquet(OUTPUT_BASE)\n",
    "        )\n",
    "\n",
    "        save_marker(spark, OUTPUT_BASE, \"ym\", m)\n",
    "\n",
    "        processed.append(m)\n",
    "        print(f\"[OK] ym={m} listo.\")\n",
    "        del df, df_t, df_part\n",
    "        spark.catalog.clearCache()\n",
    "    except Exception as e:\n",
    "        failed.append((m, str(e)))\n",
    "        print(f\"[FAIL] ym={m} -> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a037543-ec08-4b82-805b-dcfc779bd25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n========== RESUMEN ==========\")\n",
    "print(f\"Procesadas: {processed}\")\n",
    "print(f\"Saltadas:   {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446b954-2c4b-4ccb-8108-19e17cd44373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if failed:\n",
    "    print(\"Fallidas:\")\n",
    "    for m, err in failed:\n",
    "        print(f\"  - ym={m}: {err}\")\n",
    "else:\n",
    "    print(\"Fallidas:   []\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347d211-7cb8-44a7-8db1-630f5bd6a9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275a5af-a5c0-413a-a1fd-aa162781aaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
