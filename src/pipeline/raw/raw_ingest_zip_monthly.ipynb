{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9472f2b9-1c90-47e2-bf0a-6f11addf7044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name,\n",
    "    count,\n",
    "    current_timestamp,\n",
    "    to_timestamp,\n",
    "    date_format,\n",
    "    coalesce,\n",
    "    lit,\n",
    "    col,\n",
    "    regexp_extract,\n",
    ")\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ff9c0-9367-47cc-bd87-be167ad8e9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_PROJECT_ID = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "DEFAULT_BUCKET = \"bucket20250825maestria\"\n",
    "DEFAULT_ZIP_PREFIX = \"AIS_2024\"\n",
    "DEFAULT_OUT_UNZIPPED_PREFIX = \"tmp_unzipped/AIS_2024/\"\n",
    "DEFAULT_OUT_PARQUET_PREFIX = \"AIS_2024_raw/\"\n",
    "DEFAULT_CSV_OPTS = {\"header\": True}\n",
    "DEFAULT_CLEANUP_UNZIPPED = True\n",
    "\n",
    "DEFAULT_ZIP_NAME_REGEX = r\"^AIS_2024_10_.*\\.zip$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321a2dd-9d9a-4fa9-a5a2-646d40164b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = sys.argv\n",
    "if len(args) >= 6:\n",
    "    PROJECT_ID = args[1]\n",
    "    BUCKET = args[2]\n",
    "    ZIP_PREFIX = args[3]\n",
    "    OUT_UNZIPPED_PREFIX = args[4].rstrip(\"/\") + \"/\"\n",
    "    OUT_PARQUET_PREFIX = args[5].rstrip(\"/\") + \"/\"\n",
    "    CSV_OPTS = json.loads(args[6]) if len(args) > 6 else DEFAULT_CSV_OPTS\n",
    "    CLEANUP_UNZIPPED = (\n",
    "        (str(args[7]).lower() == \"true\") if len(args) > 7 else DEFAULT_CLEANUP_UNZIPPED\n",
    "    )\n",
    "else:\n",
    "    PROJECT_ID = DEFAULT_PROJECT_ID\n",
    "    BUCKET = DEFAULT_BUCKET\n",
    "    ZIP_PREFIX = DEFAULT_ZIP_PREFIX\n",
    "    OUT_UNZIPPED_PREFIX = DEFAULT_OUT_UNZIPPED_PREFIX\n",
    "    OUT_PARQUET_PREFIX = DEFAULT_OUT_PARQUET_PREFIX\n",
    "    CSV_OPTS = DEFAULT_CSV_OPTS\n",
    "    CLEANUP_UNZIPPED = DEFAULT_CLEANUP_UNZIPPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7951764-d04a-4984-9669-34609ec12171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ZIP_NAME_REGEX = os.getenv(\"ZIP_NAME_REGEX\", DEFAULT_ZIP_NAME_REGEX)\n",
    "zip_name_re = re.compile(ZIP_NAME_REGEX)\n",
    "CSV_OPTS_NORM = {\n",
    "    k: (str(v).lower() if isinstance(v, bool) else v) for k, v in CSV_OPTS.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2260ee6b-801e-43ce-bef2-685eb25f3ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _is_safe_tmp_prefix(prefix: str) -> bool:\n",
    "    return prefix.endswith(\"/\") and (\"tmp_unzipped\" in prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d00a4cc-1131-447f-a1f6-86ec24b212eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"ais-jan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c5579a-6b45-48d7-8fd8-48a84e1b2392",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/24 06:44:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "2025-09-24 06:44:21,509 INFO Spark session initialized.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"AIS-2024-10-to-parquet\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "sc = spark.sparkContext\n",
    "log.info(\"Spark session initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ce2e5-ae32-4ff0-8c55-1b7b71502a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\n",
    "    \"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "409c9658-bff8-46f8-96db-bf4fb6c43eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcs = storage.Client(project=PROJECT_ID)\n",
    "bkt = gcs.bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affc0f5-f3ba-4374-9573-a73eb0bf8538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 06:44:21,565 INFO Listing ZIPs in gs://bucket20250825maestria/AIS_2024 ...\n",
      "2025-09-24 06:44:21,714 INFO ZIPs seleccionados: ['AIS_2024/AIS_2024_10_01.zip', 'AIS_2024/AIS_2024_10_02.zip', 'AIS_2024/AIS_2024_10_03.zip', 'AIS_2024/AIS_2024_10_04.zip', 'AIS_2024/AIS_2024_10_05.zip', 'AIS_2024/AIS_2024_10_06.zip', 'AIS_2024/AIS_2024_10_07.zip', 'AIS_2024/AIS_2024_10_08.zip', 'AIS_2024/AIS_2024_10_09.zip', 'AIS_2024/AIS_2024_10_10.zip', 'AIS_2024/AIS_2024_10_11.zip', 'AIS_2024/AIS_2024_10_12.zip', 'AIS_2024/AIS_2024_10_13.zip', 'AIS_2024/AIS_2024_10_14.zip', 'AIS_2024/AIS_2024_10_15.zip', 'AIS_2024/AIS_2024_10_16.zip', 'AIS_2024/AIS_2024_10_17.zip', 'AIS_2024/AIS_2024_10_18.zip', 'AIS_2024/AIS_2024_10_19.zip', 'AIS_2024/AIS_2024_10_20.zip', 'AIS_2024/AIS_2024_10_21.zip', 'AIS_2024/AIS_2024_10_22.zip', 'AIS_2024/AIS_2024_10_23.zip', 'AIS_2024/AIS_2024_10_24.zip', 'AIS_2024/AIS_2024_10_25.zip', 'AIS_2024/AIS_2024_10_26.zip', 'AIS_2024/AIS_2024_10_27.zip', 'AIS_2024/AIS_2024_10_28.zip', 'AIS_2024/AIS_2024_10_29.zip', 'AIS_2024/AIS_2024_10_30.zip', 'AIS_2024/AIS_2024_10_31.zip']\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"Listing ZIPs in gs://{BUCKET}/{ZIP_PREFIX} ...\")\n",
    "all_zip_blob_names: List[str] = [\n",
    "    blob.name\n",
    "    for blob in gcs.list_blobs(BUCKET, prefix=ZIP_PREFIX)\n",
    "    if blob.name.lower().endswith(\".zip\")\n",
    "]\n",
    "zip_blob_names = [\n",
    "    n for n in all_zip_blob_names if zip_name_re.search(os.path.basename(n))\n",
    "]\n",
    "if not zip_blob_names:\n",
    "    raise SystemExit(\"No ZIP files matched regex\")\n",
    "log.info(f\"ZIPs seleccionados: {zip_blob_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36fc70-a731-44d0-a913-bf234a5de7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unzip_one(zip_name: str) -> tuple[str, int]:\n",
    "    import tempfile, os\n",
    "    from google.cloud import storage\n",
    "    from google.api_core import exceptions as gax_exceptions\n",
    "    import time\n",
    "\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    bkt = client.bucket(BUCKET)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        prefix=\"zip_\", suffix=\".zip\", delete=False\n",
    "    ) as tmpf:\n",
    "        tmp_path = tmpf.name\n",
    "\n",
    "    try:\n",
    "        blob = bkt.blob(zip_name)\n",
    "        blob.download_to_filename(tmp_path, timeout=60)\n",
    "\n",
    "        extracted = 0\n",
    "        zip_stem = os.path.splitext(os.path.basename(zip_name))[0]\n",
    "        base_prefix = f\"{OUT_UNZIPPED_PREFIX}{zip_stem}/\"\n",
    "\n",
    "        with ZipFile(tmp_path, \"r\") as zf:\n",
    "            for info in zf.infolist():\n",
    "                if info.is_dir():\n",
    "                    continue\n",
    "                safe = info.filename.replace(\"\\\\\", \"/\").lstrip(\"/\")\n",
    "                out_blob = bkt.blob(f\"{base_prefix}{safe}\")\n",
    "                with zf.open(info, \"r\") as member_fp:\n",
    "                    out_blob.upload_from_file(member_fp, rewind=True, timeout=60)\n",
    "                extracted += 1\n",
    "        return (zip_name, extracted)\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(tmp_path)\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14473ac-44a7-44ee-ab37-4da66121955f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 06:49:31,730 INFO Extracted 31 CSV(s).                               \n"
     ]
    }
   ],
   "source": [
    "stats = (\n",
    "    sc.parallelize(zip_blob_names, numSlices=min(4, len(zip_blob_names)))\n",
    "    .map(unzip_one)\n",
    "    .collect()\n",
    ")\n",
    "files_unzipped = int(sum(n for _, n in stats))\n",
    "log.info(f\"Extracted {files_unzipped} CSV(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46864cba-1afe-4f99-9449-c46822e7793e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"MMSI\", LongType(), True),\n",
    "        StructField(\"BaseDateTime\", StringType(), True),\n",
    "        StructField(\"LAT\", DoubleType(), True),\n",
    "        StructField(\"LON\", DoubleType(), True),\n",
    "        StructField(\"SOG\", DoubleType(), True),\n",
    "        StructField(\"COG\", DoubleType(), True),\n",
    "        StructField(\"Heading\", DoubleType(), True),\n",
    "        StructField(\"VesselName\", StringType(), True),\n",
    "        StructField(\"IMO\", StringType(), True),\n",
    "        StructField(\"CallSign\", StringType(), True),\n",
    "        StructField(\"VesselType\", StringType(), True),\n",
    "        StructField(\"Status\", StringType(), True),\n",
    "        StructField(\"Length\", DoubleType(), True),\n",
    "        StructField(\"Width\", DoubleType(), True),\n",
    "        StructField(\"Draft\", DoubleType(), True),\n",
    "        StructField(\"Cargo\", StringType(), True),\n",
    "        StructField(\"TransceiverClass\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5090cc45-1211-45a4-8921-c7a51cba1ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_gcs_paths = []\n",
    "for z in zip_blob_names:\n",
    "    stem = os.path.splitext(os.path.basename(z))[0]\n",
    "    for blob in gcs.list_blobs(BUCKET, prefix=f\"{OUT_UNZIPPED_PREFIX}{stem}/\"):\n",
    "        if blob.name.lower().endswith(\".csv\"):\n",
    "            csv_gcs_paths.append(f\"gs://{BUCKET}/{blob.name}\")\n",
    "\n",
    "if not csv_gcs_paths:\n",
    "    raise SystemExit(\"No CSVs found after unzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409cbab2-5499-4bc5-8608-da0c2c65528e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = spark.read.options(**CSV_OPTS_NORM)\n",
    "df = reader.csv(csv_gcs_paths, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7719433-b4ec-488c-a3fc-d20c03b7a2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"_source_file\", input_file_name()).withColumn(\n",
    "    \"_ingest_ts\", current_timestamp()\n",
    ")\n",
    "df = df.withColumn(\"ym\", regexp_extract(\"BaseDateTime\", r\"^(\\d{4}-\\d{2})\", 1))\n",
    "df = df.withColumn(\"ymd\", regexp_extract(\"BaseDateTime\", r\"^(\\d{4}-\\d{2}-\\d{2})\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd0f17d-1ad1-4197-be85-e8459f67cc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.persist()\n",
    "csv_row_count = df.count()\n",
    "log.info(f\"CSV row count total: {csv_row_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07842d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_parquet_uri = f\"gs://{BUCKET}/{OUT_PARQUET_PREFIX}\"\n",
    "(\n",
    "    df.write.mode(\"overwrite\")\n",
    "    .option(\"partitionOverwriteMode\", \"dynamic\")\n",
    "    .partitionBy(\"ym\")\n",
    "    .parquet(out_parquet_uri)\n",
    ")\n",
    "log.info(f\"Parquet written to {out_parquet_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba5f8f-2b64-4213-9dec-945e9c676ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yms = [r[\"ym\"] for r in df.select(\"ym\").distinct().collect()]\n",
    "ym_paths = [f\"{out_parquet_uri}/ym={ym}\" for ym in yms if ym]\n",
    "parq_df = (\n",
    "    spark.read.parquet(*ym_paths)\n",
    "    .withColumn(\"ymd\", date_format(to_timestamp(\"BaseDateTime\"), \"yyyy-MM-dd\"))\n",
    "    .persist()\n",
    ")\n",
    "parq_row_count = parq_df.count()\n",
    "log.info(f\"Parquet row count total: {parq_row_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bd59e-c1fe-4f4d-aa7c-14556b175bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_day_counts = df.groupBy(\"ymd\").count().withColumnRenamed(\"count\", \"csv_rows\")\n",
    "parq_day_counts = (\n",
    "    parq_df.groupBy(\"ymd\").count().withColumnRenamed(\"count\", \"parquet_rows\")\n",
    ")\n",
    "day_compare = (\n",
    "    csv_day_counts.join(parq_day_counts, on=\"ymd\", how=\"full\")\n",
    "    .withColumn(\"csv_rows\", coalesce(col(\"csv_rows\"), lit(0)))\n",
    "    .withColumn(\"parquet_rows\", coalesce(col(\"parquet_rows\"), lit(0)))\n",
    "    .withColumn(\"match\", col(\"csv_rows\") == col(\"parquet_rows\"))\n",
    "    .orderBy(\"ymd\")\n",
    ")\n",
    "\n",
    "rows = day_compare.collect()\n",
    "log.info(\"===== Comparación de conteos por día =====\")\n",
    "for r in rows:\n",
    "    log.info(\n",
    "        f\"{r['ymd']}: CSV={int(r['csv_rows']):,} | Parquet={int(r['parquet_rows']):,} | match={r['match']}\"\n",
    "    )\n",
    "\n",
    "mismatches = [r for r in rows if not r[\"match\"]]\n",
    "if mismatches:\n",
    "    log.error(\"¡Hay diferencias por día!\")\n",
    "    for r in mismatches:\n",
    "        log.error(\n",
    "            f\"  {r['ymd']}: CSV={int(r['csv_rows'])}, Parquet={int(r['parquet_rows'])}\"\n",
    "        )\n",
    "    raise SystemExit(2)\n",
    "else:\n",
    "    log.info(\"Validación por día PASSED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9030761-7e7c-40e1-b4de-b9575f320e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 07:23:29,378 INFO Cleaning tmp under gs://bucket20250825maestria/tmp_unzipped/AIS_2024/ ...\n"
     ]
    }
   ],
   "source": [
    "success_blob = bkt.blob(f\"{OUT_PARQUET_PREFIX}_SUCCESS\")\n",
    "if success_blob.exists():\n",
    "    success_blob.delete()\n",
    "    log.info(\"Deleted _SUCCESS file.\")\n",
    "\n",
    "if CLEANUP_UNZIPPED and _is_safe_tmp_prefix(OUT_UNZIPPED_PREFIX):\n",
    "    log.info(f\"Cleaning tmp under gs://{BUCKET}/{OUT_UNZIPPED_PREFIX} ...\")\n",
    "    for blob in gcs.list_blobs(BUCKET, prefix=OUT_UNZIPPED_PREFIX):\n",
    "        blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9b86c44-c3f1-4bbe-87a2-a54290f48e14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 07:23:35,656 INFO Job finished successfully.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "log.info(\"Job finished successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e9a10-a766-469e-aa20-f3f3df096534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
