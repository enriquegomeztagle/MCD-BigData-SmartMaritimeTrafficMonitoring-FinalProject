{"cells": [{"cell_type": "code", "execution_count": 2, "id": "9472f2b9-1c90-47e2-bf0a-6f11addf7044", "metadata": {"tags": []}, "outputs": [], "source": "import os\nimport re\nimport sys\nimport json\nimport datetime\nimport logging\nfrom typing import List\nfrom zipfile import ZipFile\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\nfrom pyspark.sql.functions import (\n    input_file_name,\n    count,\n    current_timestamp,\n    to_timestamp,\n    date_format,\n    coalesce,\n    lit,\n    col,\n    regexp_extract\n)\n\nfrom google.cloud import storage"}, {"cell_type": "code", "execution_count": 3, "id": "148ff9c0-9367-47cc-bd87-be167ad8e9d6", "metadata": {"tags": []}, "outputs": [], "source": "DEFAULT_PROJECT_ID = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\nDEFAULT_BUCKET = \"bucket20250825maestria\"\nDEFAULT_ZIP_PREFIX = \"AIS_2024\"\nDEFAULT_OUT_UNZIPPED_PREFIX = \"tmp_unzipped/AIS_2024/\"\nDEFAULT_OUT_PARQUET_PREFIX = \"AIS_2024_processed_bronze/\"\nDEFAULT_CSV_OPTS = {\"header\": True}\nDEFAULT_CLEANUP_UNZIPPED = True\n\nDEFAULT_ZIP_NAME_REGEX = r\"^AIS_2024_10_.*\\.zip$\""}, {"cell_type": "code", "execution_count": 4, "id": "e321a2dd-9d9a-4fa9-a5a2-646d40164b45", "metadata": {"tags": []}, "outputs": [], "source": "args = sys.argv\nif len(args) >= 6:\n    PROJECT_ID = args[1]\n    BUCKET = args[2]\n    ZIP_PREFIX = args[3]\n    OUT_UNZIPPED_PREFIX = args[4].rstrip(\"/\") + \"/\"\n    OUT_PARQUET_PREFIX = args[5].rstrip(\"/\") + \"/\"\n    CSV_OPTS = json.loads(args[6]) if len(args) > 6 else DEFAULT_CSV_OPTS\n    CLEANUP_UNZIPPED = (str(args[7]).lower() == \"true\") if len(args) > 7 else DEFAULT_CLEANUP_UNZIPPED\nelse:\n    PROJECT_ID = DEFAULT_PROJECT_ID\n    BUCKET = DEFAULT_BUCKET\n    ZIP_PREFIX = DEFAULT_ZIP_PREFIX\n    OUT_UNZIPPED_PREFIX = DEFAULT_OUT_UNZIPPED_PREFIX\n    OUT_PARQUET_PREFIX = DEFAULT_OUT_PARQUET_PREFIX\n    CSV_OPTS = DEFAULT_CSV_OPTS\n    CLEANUP_UNZIPPED = DEFAULT_CLEANUP_UNZIPPED"}, {"cell_type": "code", "execution_count": 5, "id": "a7951764-d04a-4984-9669-34609ec12171", "metadata": {"tags": []}, "outputs": [], "source": "ZIP_NAME_REGEX = os.getenv(\"ZIP_NAME_REGEX\", DEFAULT_ZIP_NAME_REGEX)\nzip_name_re = re.compile(ZIP_NAME_REGEX)\nCSV_OPTS_NORM = {k: (str(v).lower() if isinstance(v, bool) else v) for k, v in CSV_OPTS.items()}"}, {"cell_type": "code", "execution_count": 6, "id": "2260ee6b-801e-43ce-bef2-685eb25f3ce3", "metadata": {"tags": []}, "outputs": [], "source": "def _is_safe_tmp_prefix(prefix: str) -> bool:\n    return prefix.endswith(\"/\") and (\"tmp_unzipped\" in prefix)"}, {"cell_type": "code", "execution_count": 7, "id": "8d00a4cc-1131-447f-a1f6-86ec24b212eb", "metadata": {"tags": []}, "outputs": [], "source": "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlog = logging.getLogger(\"ais-jan\")"}, {"cell_type": "code", "execution_count": 8, "id": "b3c5579a-6b45-48d7-8fd8-48a84e1b2392", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/09/24 06:44:21 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n2025-09-24 06:44:21,509 INFO Spark session initialized.\n"}], "source": "spark = SparkSession.builder.appName(\"AIS-2024-10-to-parquet\").getOrCreate()\nspark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\nsc = spark.sparkContext\nlog.info(\"Spark session initialized.\")"}, {"cell_type": "code", "execution_count": 9, "id": "dd9ce2e5-ae32-4ff0-8c55-1b7b71502a7e", "metadata": {"tags": []}, "outputs": [], "source": "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"}, {"cell_type": "code", "execution_count": 10, "id": "409c9658-bff8-46f8-96db-bf4fb6c43eaa", "metadata": {"tags": []}, "outputs": [], "source": "gcs = storage.Client(project=PROJECT_ID)\nbkt = gcs.bucket(BUCKET)"}, {"cell_type": "code", "execution_count": 11, "id": "7affc0f5-f3ba-4374-9573-a73eb0bf8538", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2025-09-24 06:44:21,565 INFO Listing ZIPs in gs://bucket20250825maestria/AIS_2024 ...\n2025-09-24 06:44:21,714 INFO ZIPs seleccionados: ['AIS_2024/AIS_2024_10_01.zip', 'AIS_2024/AIS_2024_10_02.zip', 'AIS_2024/AIS_2024_10_03.zip', 'AIS_2024/AIS_2024_10_04.zip', 'AIS_2024/AIS_2024_10_05.zip', 'AIS_2024/AIS_2024_10_06.zip', 'AIS_2024/AIS_2024_10_07.zip', 'AIS_2024/AIS_2024_10_08.zip', 'AIS_2024/AIS_2024_10_09.zip', 'AIS_2024/AIS_2024_10_10.zip', 'AIS_2024/AIS_2024_10_11.zip', 'AIS_2024/AIS_2024_10_12.zip', 'AIS_2024/AIS_2024_10_13.zip', 'AIS_2024/AIS_2024_10_14.zip', 'AIS_2024/AIS_2024_10_15.zip', 'AIS_2024/AIS_2024_10_16.zip', 'AIS_2024/AIS_2024_10_17.zip', 'AIS_2024/AIS_2024_10_18.zip', 'AIS_2024/AIS_2024_10_19.zip', 'AIS_2024/AIS_2024_10_20.zip', 'AIS_2024/AIS_2024_10_21.zip', 'AIS_2024/AIS_2024_10_22.zip', 'AIS_2024/AIS_2024_10_23.zip', 'AIS_2024/AIS_2024_10_24.zip', 'AIS_2024/AIS_2024_10_25.zip', 'AIS_2024/AIS_2024_10_26.zip', 'AIS_2024/AIS_2024_10_27.zip', 'AIS_2024/AIS_2024_10_28.zip', 'AIS_2024/AIS_2024_10_29.zip', 'AIS_2024/AIS_2024_10_30.zip', 'AIS_2024/AIS_2024_10_31.zip']\n"}], "source": "log.info(f\"Listing ZIPs in gs://{BUCKET}/{ZIP_PREFIX} ...\")\nall_zip_blob_names: List[str] = [\n    blob.name\n    for blob in gcs.list_blobs(BUCKET, prefix=ZIP_PREFIX)\n    if blob.name.lower().endswith(\".zip\")\n]\nzip_blob_names = [n for n in all_zip_blob_names if zip_name_re.search(os.path.basename(n))]\nif not zip_blob_names:\n    raise SystemExit(\"No ZIP files matched regex\")\nlog.info(f\"ZIPs seleccionados: {zip_blob_names}\")"}, {"cell_type": "code", "execution_count": 12, "id": "8d36fc70-a731-44d0-a913-bf234a5de7e8", "metadata": {"tags": []}, "outputs": [], "source": "def unzip_one(zip_name: str) -> tuple[str, int]:\n    import tempfile, os\n    from google.cloud import storage\n    from google.api_core import exceptions as gax_exceptions\n    import time\n\n    client = storage.Client(project=PROJECT_ID)\n    bkt = client.bucket(BUCKET)\n\n    with tempfile.NamedTemporaryFile(prefix=\"zip_\", suffix=\".zip\", delete=False) as tmpf:\n        tmp_path = tmpf.name\n\n    try:\n        blob = bkt.blob(zip_name)\n        blob.download_to_filename(tmp_path, timeout=60)\n\n        extracted = 0\n        zip_stem = os.path.splitext(os.path.basename(zip_name))[0]\n        base_prefix = f\"{OUT_UNZIPPED_PREFIX}{zip_stem}/\"\n\n        with ZipFile(tmp_path, \"r\") as zf:\n            for info in zf.infolist():\n                if info.is_dir():\n                    continue\n                safe = info.filename.replace(\"\\\\\", \"/\").lstrip(\"/\")\n                out_blob = bkt.blob(f\"{base_prefix}{safe}\")\n                with zf.open(info, \"r\") as member_fp:\n                    out_blob.upload_from_file(member_fp, rewind=True, timeout=60)\n                extracted += 1\n        return (zip_name, extracted)\n    finally:\n        try:\n            os.remove(tmp_path)\n        except Exception:\n            pass"}, {"cell_type": "code", "execution_count": 13, "id": "f14473ac-44a7-44ee-ab37-4da66121955f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2025-09-24 06:49:31,730 INFO Extracted 31 CSV(s).                               \n"}], "source": "stats = sc.parallelize(zip_blob_names, numSlices=min(4, len(zip_blob_names))).map(unzip_one).collect()\nfiles_unzipped = int(sum(n for _, n in stats))\nlog.info(f\"Extracted {files_unzipped} CSV(s).\")"}, {"cell_type": "code", "execution_count": 14, "id": "46864cba-1afe-4f99-9449-c46822e7793e", "metadata": {"tags": []}, "outputs": [], "source": "schema = StructType([\n    StructField(\"MMSI\", LongType(), True),\n    StructField(\"BaseDateTime\", StringType(), True),\n    StructField(\"LAT\", DoubleType(), True),\n    StructField(\"LON\", DoubleType(), True),\n    StructField(\"SOG\", DoubleType(), True),\n    StructField(\"COG\", DoubleType(), True),\n    StructField(\"Heading\", DoubleType(), True),\n    StructField(\"VesselName\", StringType(), True),\n    StructField(\"IMO\", StringType(), True),\n    StructField(\"CallSign\", StringType(), True),\n    StructField(\"VesselType\", StringType(), True),\n    StructField(\"Status\", StringType(), True),\n    StructField(\"Length\", DoubleType(), True),\n    StructField(\"Width\", DoubleType(), True),\n    StructField(\"Draft\", DoubleType(), True),\n    StructField(\"Cargo\", StringType(), True),\n    StructField(\"TransceiverClass\", StringType(), True),\n])"}, {"cell_type": "code", "execution_count": 15, "id": "5090cc45-1211-45a4-8921-c7a51cba1ccc", "metadata": {"tags": []}, "outputs": [], "source": "csv_gcs_paths = []\nfor z in zip_blob_names:\n    stem = os.path.splitext(os.path.basename(z))[0]\n    for blob in gcs.list_blobs(BUCKET, prefix=f\"{OUT_UNZIPPED_PREFIX}{stem}/\"):\n        if blob.name.lower().endswith(\".csv\"):\n            csv_gcs_paths.append(f\"gs://{BUCKET}/{blob.name}\")\n\nif not csv_gcs_paths:\n    raise SystemExit(\"No CSVs found after unzip\")"}, {"cell_type": "code", "execution_count": 16, "id": "409cbab2-5499-4bc5-8608-da0c2c65528e", "metadata": {"tags": []}, "outputs": [], "source": "reader = spark.read.options(**CSV_OPTS_NORM)\ndf = reader.csv(csv_gcs_paths, schema=schema)"}, {"cell_type": "code", "execution_count": 17, "id": "e7719433-b4ec-488c-a3fc-d20c03b7a2ac", "metadata": {"tags": []}, "outputs": [], "source": "df = df.withColumn(\"_source_file\", input_file_name()).withColumn(\"_ingest_ts\", current_timestamp())\ndf = df.withColumn(\"ym\", regexp_extract(\"BaseDateTime\", r\"^(\\d{4}-\\d{2})\", 1))\ndf = df.withColumn(\"ymd\", regexp_extract(\"BaseDateTime\", r\"^(\\d{4}-\\d{2}-\\d{2})\", 1))"}, {"cell_type": "code", "execution_count": 18, "id": "7bd0f17d-1ad1-4197-be85-e8459f67cc3a", "metadata": {"tags": []}, "outputs": [], "source": "df = df.persist()\ncsv_row_count = df.count()\nlog.info(f\"CSV row count total: {csv_row_count:,}\")"}, {"cell_type": "code", "execution_count": 19, "id": "c5167a51-f772-43ba-8b9e-1f3cf1984a75", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2025-09-24 07:04:39,714 INFO Parquet written to gs://bucket20250825maestria/AIS_2024_processed_bronze/\n"}], "source": "out_parquet_uri = f\"gs://{BUCKET}/{OUT_PARQUET_PREFIX}\"\n(df.write\n .mode(\"overwrite\")\n .option(\"partitionOverwriteMode\", \"dynamic\")\n .partitionBy(\"ym\")\n .parquet(out_parquet_uri))\nlog.info(f\"Parquet written to {out_parquet_uri}\")"}, {"cell_type": "code", "execution_count": null, "id": "8bba5f8f-2b64-4213-9dec-945e9c676ed3", "metadata": {"tags": []}, "outputs": [], "source": "yms = [r[\"ym\"] for r in df.select(\"ym\").distinct().collect()]\nym_paths = [f\"{out_parquet_uri}/ym={ym}\" for ym in yms if ym]\nparq_df = spark.read.parquet(*ym_paths).withColumn(\"ymd\", date_format(to_timestamp(\"BaseDateTime\"), \"yyyy-MM-dd\")).persist()\nparq_row_count = parq_df.count()\nlog.info(f\"Parquet row count total: {parq_row_count:,}\")"}, {"cell_type": "code", "execution_count": null, "id": "1f6bd59e-c1fe-4f4d-aa7c-14556b175bd9", "metadata": {"tags": []}, "outputs": [], "source": "csv_day_counts = df.groupBy(\"ymd\").count().withColumnRenamed(\"count\", \"csv_rows\")\nparq_day_counts = parq_df.groupBy(\"ymd\").count().withColumnRenamed(\"count\", \"parquet_rows\")\nday_compare = (\n    csv_day_counts.join(parq_day_counts, on=\"ymd\", how=\"full\")\n                  .withColumn(\"csv_rows\", coalesce(col(\"csv_rows\"), lit(0)))\n                  .withColumn(\"parquet_rows\", coalesce(col(\"parquet_rows\"), lit(0)))\n                  .withColumn(\"match\", col(\"csv_rows\") == col(\"parquet_rows\"))\n                  .orderBy(\"ymd\")\n)\n\nrows = day_compare.collect()\nlog.info(\"===== Comparaci\u00f3n de conteos por d\u00eda =====\")\nfor r in rows:\n    log.info(f\"{r['ymd']}: CSV={int(r['csv_rows']):,} | Parquet={int(r['parquet_rows']):,} | match={r['match']}\")\n\nmismatches = [r for r in rows if not r[\"match\"]]\nif mismatches:\n    log.error(\"\u00a1Hay diferencias por d\u00eda!\")\n    for r in mismatches:\n        log.error(f\"  {r['ymd']}: CSV={int(r['csv_rows'])}, Parquet={int(r['parquet_rows'])}\")\n    raise SystemExit(2)\nelse:\n    log.info(\"Validaci\u00f3n por d\u00eda PASSED.\")"}, {"cell_type": "code", "execution_count": 22, "id": "a9030761-7e7c-40e1-b4de-b9575f320e4e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2025-09-24 07:23:29,378 INFO Cleaning tmp under gs://bucket20250825maestria/tmp_unzipped/AIS_2024/ ...\n"}], "source": "success_blob = bkt.blob(f\"{OUT_PARQUET_PREFIX}_SUCCESS\")\nif success_blob.exists():\n    success_blob.delete()\n    log.info(\"Deleted _SUCCESS file.\")\n\nif CLEANUP_UNZIPPED and _is_safe_tmp_prefix(OUT_UNZIPPED_PREFIX):\n    log.info(f\"Cleaning tmp under gs://{BUCKET}/{OUT_UNZIPPED_PREFIX} ...\")\n    for blob in gcs.list_blobs(BUCKET, prefix=OUT_UNZIPPED_PREFIX):\n        blob.delete()"}, {"cell_type": "code", "execution_count": 23, "id": "a9b86c44-c3f1-4bbe-87a2-a54290f48e14", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "2025-09-24 07:23:35,656 INFO Job finished successfully.\n"}], "source": "spark.stop()\nlog.info(\"Job finished successfully.\")"}, {"cell_type": "code", "execution_count": null, "id": "b21e9a10-a766-469e-aa20-f3f3df096534", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.13"}}, "nbformat": 4, "nbformat_minor": 5}